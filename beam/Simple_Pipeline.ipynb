{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Pipeline",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dangro/gcp-playground/blob/master/beam/Simple_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PiMU50AlNBbf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting started with Apache Beam\n",
        "\n",
        "In this notebook I create a very simple Apache Beam pipeline that will read and write from and to Google Cloud Storage.   \n",
        "This pipeline is not meant to execute in Google Cloud Platform and will use a local runner instead.\n",
        "\n",
        "For a sufficiently large data set or a complex pipeline we want to take advantage of Cloud Dataflow.\n",
        "\n",
        "To get started you will need a GCP project and a GCS bucket.  \n",
        "Please updload a text file to the GCS bucket to use in this pipeline."
      ]
    },
    {
      "metadata": {
        "id": "8OHlav8eMRgE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Install dependencies\n",
        "\n",
        "Will be using Google Cloud Storage to read and write data used in the pipeline we need to installl package ```apache-beam[gcp]```."
      ]
    },
    {
      "metadata": {
        "id": "5911dRMuIhqD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install apache-beam[gcp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0CW835iOPqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Configure the ```gcloud``` CLI\n",
        "If you haven't already, create a project in Google Cloud Platform.\n",
        "\n",
        "In the cell below replace:\n",
        "1. ```GCP_PROJECT_NAME``` with the name of your project.\n",
        "2. ```GCP_BUCKET_NAME``` with the name of the bucket to read from and write data to in this pipeline.\n",
        "\n",
        "We will use this later on to get output of the pipeline."
      ]
    },
    {
      "metadata": {
        "id": "xEN2mKsXMBSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%env PROJECT_NAME=GCP_PROJECT_NAME\n",
        "%env BUCKET_NAME=GCP_BUCKET_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2kc8HzvHnKk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gcloud config set project $PROJECT_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odhb-YblaC6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from google.colab import auth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5xPLsOXcDRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Authenticate the notebook to use Google Cloud Storage"
      ]
    },
    {
      "metadata": {
        "id": "1x7e3SltH9Uz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJTMwJwfFC85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Set the name of the GCS bucket\n",
        "\n",
        "If you haven't already, create a bucket in Google Cloud Storage. This bucket may be a regional bucket.\n",
        "\n",
        "In the cell below replace:\n",
        "1. ```GCP_BUCKET_NAME``` with the name of your bucket. This will be used by the pipeline to read and write data.\n",
        "2. ```INPUT_FILE_NAME``` with the name of the file you uploaded to the bucket.\n",
        "3. Optionally, change the value of ```SPLIT_ON_SEQUENCE``` with a sequence of characters to split the input."
      ]
    },
    {
      "metadata": {
        "id": "LQMM-FREDmeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = \"GCP_BUCKET_NAME\"\n",
        "INPUT_FILE_IN_BUCKET = \"INPUT_FILE_NAME\"\n",
        "SPLIT_ON_SEQUENCE = \" \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Rl_ItSpSv2h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Helper functions to create a pipeline"
      ]
    },
    {
      "metadata": {
        "id": "sXyBPa7kEeS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_gcs_path(bucket, file):\n",
        "  \"\"\"Helper function that returns a GCS path\"\"\"\n",
        "  return \"gs://\" + bucket + \"/\" + file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8UcqhhrHaFXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SplitString(beam.DoFn):\n",
        "  \"\"\"Splits input on the character(s) defined in SPLIT_ON_SEQUENCE\"\"\"\n",
        "  def process(self, element):\n",
        "    yield element.split(SPLIT_ON_SEQUENCE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZVosUshbJ3wZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_pipeline(input_bucket, output_bucket):\n",
        "  \"\"\"Returns an Apache Beam pipeline\"\"\"\n",
        "  pipeline = beam.Pipeline()\n",
        "  text = (pipeline\n",
        "      | 'read' >> ReadFromText(input_bucket)\n",
        "      | 'split' >> beam.ParDo(SplitString())\n",
        "      | 'write' >> WriteToText(output_bucket))\n",
        "  return pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XXVemi4mc70q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function ```make_pipeline``` creates a Beam Pipeline that loads data from GCS, applies transforms to the data, and finally writes the output back to GCS.\n",
        "1. [ReadFromText](https://beam.apache.org/releases/pydoc/2.11.0/apache_beam.io.textio.html?highlight=readfromtext#apache_beam.io.textio.ReadFromText): The result is a PCollection, that is, an object that references the data that will be processed by the pipeline.\n",
        "2. [ParDo](https://beam.apache.org/releases/pydoc/2.11.0/apache_beam.transforms.core.html#apache_beam.transforms.core.ParDo): ParDo is a special kind of transform that accepts objects that extend DoFn such as ```SplitString```.\n",
        "3. [WriteToText](https://beam.apache.org/releases/pydoc/2.11.0/apache_beam.io.textio.html?highlight=readfromtext#apache_beam.io.textio.WriteToText): As the pipeline finishes processing data the output is written to one or more files."
      ]
    },
    {
      "metadata": {
        "id": "MKEWGvkuaT5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_pipeline(argv=None):\n",
        "  \"\"\"Sets up and runs an Apache Beam Pipeline\"\"\"\n",
        "  INPUT_DATA = make_gcs_path(BUCKET_NAME, INPUT_FILE_IN_BUCKET)\n",
        "  OUTPUT_DATA = make_gcs_path(BUCKET_NAME, \"output\")\n",
        "  \n",
        "  pipeline = make_pipeline(INPUT_DATA, OUTPUT_DATA)\n",
        "  result = pipeline.run()\n",
        "  result.wait_until_finish()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u7zPIOnKSqFN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Run the pipeline"
      ]
    },
    {
      "metadata": {
        "id": "eRSK79NfbOR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run_pipeline()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dhXAyHUJK0dV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Explore the pipeline output\n",
        "Now that the pipeline has run and finished we may download the contents of the bucket."
      ]
    },
    {
      "metadata": {
        "id": "kHSWnaW9bVla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://$BUCKET_NAME/output-* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaSmbfY2JDmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat output-*"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}